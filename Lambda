#!/usr/bin/env python3
"""
Dual-Path Convergence: KL Drift vs Gain Control (Lambda Calculus Variant)

Integrates Variational Free Energy minimization with Functional Programming 
primitives. State updates are handled via immutable lambda transitions rather 
than imperative mutation.
"""

import random
import math
import tkinter as tk
from typing import List, Tuple, Callable, Any

# ========== Configuration ==========
STEPS: int = 100
LEARNING_RATE: float = 0.08
TRUE_MEAN: float = 2.0
TRUE_STD: float = 1.0
SAMPLE_SIZE: int = 80
DECAY_RATE: float = 0.985

# Display settings
WINDOW_WIDTH: int = 850
WINDOW_HEIGHT: int = 700
PLOT_HEIGHT: int = 160
UPDATE_DELAY: int = 50 

# ========== Lambda Calculus Primitives ==========
# We treat state as immutable tuples. The 'logic' acts as a transition function.

# Identity Combinator (I)
I = lambda x: x

# The Purification Lambda: λtheta. λgrad. theta'
# Pure geometric correction (Drift minimization)
LAMBDA_PURIFY = lambda theta, grad: theta - (LEARNING_RATE * grad)

# The Annealing Lambda: λstate. λgrad. state'
# Where state is a pair (theta, alpha)
# Returns new pair (theta', alpha')
LAMBDA_ANNEAL = lambda state, grad: (
    state[0] - (LEARNING_RATE * state[1] * grad), # Update Theta using gain (alpha)
    max(0.001, state[1] * DECAY_RATE)             # Recursively decay alpha
)

# ========== Math Utilities ==========
def mean(values: List[float]) -> float:
    return sum(values) / len(values) if values else 0.0

def variance(values: List[float], mean_val: float) -> float:
    if len(values) < 2: return 1.0
    return sum((x - mean_val) ** 2 for x in values) / (len(values) - 1)

def kl_divergence(m1: float, s1: float, m2: float, s2: float) -> float:
    """Calculates KL(N1 || N2)"""
    if s1 <= 0 or s2 <= 0: return 0.0
    return math.log(s2/s1) + (s1*s1 + (m1-m2)**2)/(2*s2*s2) - 0.5

# ========== Simulation State (Functional Wrapper) ==========
class SimulationState:
    """
    Manages state via functional transitions.
    """
    
    def __init__(self) -> None:
        # Initial States
        self.theta_purify: float = 0.0
        
        # Anneal state is a Pair (Theta, Alpha)
        self.state_anneal: Tuple[float, float] = (0.0, 1.0) 
        
        self.sigma: float = 1.0 # Constant for this simulation
        
        # History tracking (Side effects for visualization only)
        self.kl_purify: List[float] = []
        self.kl_anneal: List[float] = []
        self.loss_purify: List[float] = []
        self.loss_anneal: List[float] = []
        self.alpha_history: List[float] = []
        
    def step(self, step_num: int) -> None:
        # 1. Perception (Input Data)
        data = [random.gauss(TRUE_MEAN, TRUE_STD) for _ in range(SAMPLE_SIZE)]
        sample_mean = mean(data)
        
        # 2. Compute Gradients (The "Error Signal")
        # dF/dTheta approx (Theta - Mu)
        grad_p = self.theta_purify - sample_mean
        grad_a = self.state_anneal[0] - sample_mean
        
        # 3. Apply Lambda Transitions (The Logic Integration)
        
        # PATH A: Representation Purification
        # Apply λ_purify
        self.theta_purify = LAMBDA_PURIFY(self.theta_purify, grad_p)
        
        # PATH B: Sensitivity Annealing
        # Apply λ_anneal to the tuple state
        self.state_anneal = LAMBDA_ANNEAL(self.state_anneal, grad_a)
        
        # Unpack for readability/logging
        theta_a_current, alpha_current = self.state_anneal
        
        # 4. Measurement (Metrics)
        kl_p = kl_divergence(self.theta_purify, self.sigma, TRUE_MEAN, TRUE_STD)
        kl_a = kl_divergence(theta_a_current, self.sigma, TRUE_MEAN, TRUE_STD)
        
        loss_p = mean([(x - self.theta_purify)**2 for x in data])
        loss_a = mean([(x - theta_a_current)**2 for x in data])
        
        # 5. Logging
        self.kl_purify.append(kl_p)
        self.kl_anneal.append(kl_a)
        self.loss_purify.append(loss_p)
        self.loss_anneal.append(loss_a)
        self.alpha_history.append(alpha_current)

# ========== Visualization Components (Tkinter) ==========
class PlotCanvas:
    def __init__(self, parent: tk.Frame, title: str, labels: List[str], colors: List[str]) -> None:
        self.title = title
        self.labels = labels
        self.colors = colors
        self.frame = tk.Frame(parent, bg="#ffffff")
        self.frame.pack(fill=tk.BOTH, expand=True, pady=(0, 10))
        
        tk.Label(self.frame, text=title, font=("Arial", 11, "bold"), bg="#ffffff", fg="#334155").pack(anchor="w")
        self.canvas = tk.Canvas(self.frame, width=WINDOW_WIDTH-40, height=PLOT_HEIGHT, bg="#f8fafc", highlightthickness=1, highlightbackground="#cbd5e1")
        self.canvas.pack(pady=(5, 0))
    
    def draw(self, data_series: List[List[float]]) -> None:
        self.canvas.delete("all")
        if not data_series or not data_series[0]: return
        
        width, height = WINDOW_WIDTH - 40, PLOT_HEIGHT
        pad = 30
        p_w, p_h = width - 2*pad, height - 2*pad
        
        all_vals = [v for s in data_series for v in s]
        if not all_vals: return
        min_v, max_v = min(all_vals), max(all_vals)
        rng = max(1e-9, max_v - min_v)
        
        # Grid/Axes
        self.canvas.create_line(pad, height-pad, width-pad, height-pad, fill="#94a3b8")
        self.canvas.create_line(pad, pad, pad, height-pad, fill="#94a3b8")
        for i in range(5):
            y = pad + (p_h * i / 4)
            self.canvas.create_line(pad, y, width-pad, y, fill="#e2e8f0", dash=(2, 2))
            
        # Series
        for series, color in zip(data_series, self.colors):
            if len(series) < 2: continue
            points = []
            for i, val in enumerate(series):
                x = pad + (p_w * i / (STEPS - 1))
                y = height - pad - ((val - min_v) / rng * p_h)
                points.extend([x, max(pad, min(height-pad, y))])
            self.canvas.create_line(points, fill=color, width=2, smooth=True)
            
        # Labels
        self.canvas.create_text(pad-5, pad, text=f"{max_v:.2f}", anchor="e", font=("Arial", 8), fill="#64748b")
        self.canvas.create_text(pad-5, height-pad, text=f"{min_v:.2f}", anchor="e", font=("Arial", 8), fill="#64748b")
        
        # Legend
        if len(self.labels) > 1:
            lx, ly = width-pad-120, pad+10
            for lbl, col in zip(self.labels, self.colors):
                self.canvas.create_line(lx, ly, lx+25, ly, fill=col, width=2)
                self.canvas.create_text(lx+30, ly, text=lbl, anchor="w", font=("Arial", 9), fill="#475569")
                ly += 18

class SimulationWindow:
    def __init__(self, root: tk.Tk) -> None:
        self.root = root
        self.root.title("Dual-Path Lambda: KL Drift vs Gain Control")
        self.root.geometry(f"{WINDOW_WIDTH}x{WINDOW_HEIGHT}")
        self.root.configure(bg="#ffffff")
        
        tk.Label(root, text="Dual-Path Lambda: KL Drift vs Gain Control", font=("Arial", 16, "bold"), bg="#ffffff", fg="#1e293b").pack(pady=(15, 15))
        
        self.status_frame = tk.Frame(root, bg="#f8fafc", relief=tk.RIDGE, bd=1)
        self.status_frame.pack(fill=tk.X, padx=20, pady=(0, 15))
        
        self.lbl_step = tk.Label(self.status_frame, text="Step: 0", font=("Arial", 11, "bold"), bg="#f8fafc")
        self.lbl_step.pack(side=tk.LEFT, padx=20, pady=10)
        self.lbl_metrics = tk.Label(self.status_frame, text="", font=("Consolas", 10), bg="#f8fafc")
        self.lbl_metrics.pack(side=tk.LEFT, padx=20)
        
        self.kl_plot = PlotCanvas(root, "Information Drift (KL Divergence)", ["Purification (Drift)", "Annealing (Gain)"], ["#dc2626", "#2563eb"])
        self.loss_plot = PlotCanvas(root, "Prediction Loss (MSE)", ["Purification", "Annealing"], ["#dc2626", "#2563eb"])
        self.alpha_plot = PlotCanvas(root, "Gain Parameter α (Lambda State)", ["Alpha"], ["#16a34a"])
        
        self.state = SimulationState()
        self.step_count = 0
        self.running = True
        self.root.after(100, self.run_step)
        
    def run_step(self) -> None:
        if not self.running or self.step_count >= STEPS: return
        self.step_count += 1
        self.state.step(self.step_count)
        
        # Update UI
        self.lbl_step.config(text=f"Step: {self.step_count}/{STEPS}")
        self.lbl_metrics.config(text=f"θ_p: {self.state.theta_purify:.3f} | θ_a: {self.state.state_anneal[0]:.3f} | α: {self.state.state_anneal[1]:.3f}")
        
        self.kl_plot.draw([self.state.kl_purify, self.state.kl_anneal])
        self.loss_plot.draw([self.state.loss_purify, self.state.loss_anneal])
        self.alpha_plot.draw([self.state.alpha_history])
        
        self.root.after(UPDATE_DELAY, self.run_step)

if __name__ == "__main__":
    root = tk.Tk()
    app = SimulationWindow(root)
    root.mainloop()
